# æ¨¡å‹è®­ç»ƒè„šæœ¬ï¼šPPOç®—æ³•åœ¨OCCTç¯å¢ƒä¸­çš„å®ç°
"""
This script reproduces the Proximal Policy Optimization (PPO) Algorithm.
é€‚é…ï¼šç¯å¢ƒå†…éƒ¨è‡ªå®šä¹‰VecNormè§‚æµ‹å½’ä¸€åŒ–ï¼Œç§»é™¤åŒé‡å½’ä¸€åŒ–ï¼Œå¢å¼ºCheckpointå…¼å®¹æ€§
å¹¶è¡Œç‰ˆï¼šæ”¯æŒ Windows/Linuxï¼Œå›ºå®š VecNorm ç»Ÿè®¡é‡
"""
from __future__ import annotations
import sys
import os
import numpy as np
import functools # æ–°å¢ï¼šç”¨äºæ„é€ å¯åºåˆ—åŒ–çš„ç¯å¢ƒå‡½æ•°
import torch.multiprocessing as mp # æ–°å¢ï¼šå¤šè¿›ç¨‹ç®¡ç†

# æ·»åŠ æœ¬åœ°è‡ªå®šä¹‰torchrlçš„æ ¹ç›®å½•ï¼ˆE:\rl\torchrl\çš„ä¸Šä¸€çº§ç›®å½•ï¼Œå³E:\rl\ï¼‰
sys.path.insert(0, "E:\\rl")  # insert(0)è¡¨ç¤ºå°†è¯¥è·¯å¾„æ”¾åœ¨æœç´¢ä¼˜å…ˆçº§ç¬¬1ä½
# sys.path.insert(0, "/home/yons/Graduation/rl")  # Linuxç³»ç»Ÿä¸­çš„è·¯å¾„
# TIspdizNBNWYgfoUxNl86    swanlab API Key

import warnings

import hydra
import torchrl
from torchrl._utils import compile_with_warmup
from torchrl.record.loggers.swanlab import SwanLabLogger
import gymnasium as gym
from occt_2d2c import TwoCarrierEnv
from omegaconf import DictConfig
from torchrl.collectors import Collector
from torchrl.envs import ParallelEnv # æ–°å¢ï¼šå¹¶è¡Œç¯å¢ƒåŒ…è£…å™¨

# å¯¼å…¥å·¥å…·å‡½æ•°
from utils_ppo_occt import eval_model, make_env, make_ppo_models


# =========================================================================
# å…³é”®ä¿®æ”¹ï¼šå®šä¹‰æ¨¡å—çº§è¾…åŠ©å‡½æ•° (å¿…é¡»åœ¨ main ä¹‹å¤–ï¼Œå¦åˆ™ Windows æŠ¥é”™)
# =========================================================================
def make_train_env_wrapper(env_name, device, fixed_mean, fixed_var, shared_w_force):
    """
    ç”¨äºåœ¨å­è¿›ç¨‹ä¸­åˆ›å»ºç¯å¢ƒçš„åŒ…è£…å‡½æ•°ã€‚
    æ ¹æ®ç”¨æˆ·éœ€æ±‚ï¼Œå¹¶è¡Œè¿è¡Œæ—¶ VecNorm æ˜¯å›ºå®šçš„ (frozen=True)ã€‚
    """
    return make_env(
        env_name,
        device=device,  # å­è¿›ç¨‹é€šå¸¸å»ºè®®ç”¨ CPUï¼Œç”± Collector ç»Ÿä¸€ä¼ åˆ° GPU
        shared_w_force=shared_w_force,
        vecnorm_frozen=True,  # ç”¨æˆ·æŒ‡å®šï¼šå›ºå®šç»Ÿè®¡é‡
        vecnorm_mean=fixed_mean,
        vecnorm_var=fixed_var
        # vecnorm_frozen=False,  # <--- å…³é”®ï¼šè§£å†»ï¼Œå¼€å§‹ç»Ÿè®¡
        # vecnorm_mean=None,     # <--- å…³é”®ï¼šæ¸…ç©ºæ—§å‡å€¼
        # vecnorm_var=None       # <--- å…³é”®ï¼šæ¸…ç©ºæ—§æ–¹å·®
    )


@hydra.main(config_path="", config_name="config_occt", version_base="1.1")
def main(cfg: DictConfig):

    import torch.optim
    import tqdm

    from tensordict import TensorDict
    from tensordict.nn import CudaGraphModule

    from torchrl._utils import timeit
    from torchrl.data import LazyTensorStorage, TensorDictReplayBuffer
    from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement
    from torchrl.envs import ExplorationType, set_exploration_type
    from torchrl.objectives import ClipPPOLoss, group_optimizers
    from torchrl.objectives.value.advantages import GAE
    from torchrl.record.loggers import generate_exp_name, get_logger

    torch.set_float32_matmul_precision("high")

    device = cfg.optim.device
    if device in ("", None):
        if torch.cuda.is_available():
            device = "cuda:0"
        else:
            device = "cpu"
    device = torch.device(device)

    # å¹¶è¡Œç¯å¢ƒæ•°é‡é…ç½® (ä»cfgè¯»å–ï¼Œé»˜è®¤ä¸º4)
    num_envs = getattr(cfg.env, "num_envs", 4)
    # num_envs = 1
    print(f"ğŸš€ å¯åŠ¨å¹¶è¡Œè®­ç»ƒ | å¹¶è¡Œç¯å¢ƒæ•°: {num_envs} | å¹³å°: {os.name}")

    num_mini_batches = cfg.collector.frames_per_batch // cfg.loss.mini_batch_size
    total_network_updates = (
        (cfg.collector.total_frames // cfg.collector.frames_per_batch)
        * cfg.loss.ppo_epochs
        * num_mini_batches
    )

    compile_mode = None
    if cfg.compile.compile:
        compile_mode = cfg.compile.compile_mode
        if compile_mode in ("", None):
            if cfg.compile.cudagraphs:
                compile_mode = "default"
            else:
                compile_mode = "reduce-overhead"

    # Create models
    actor, critic = make_ppo_models(cfg.env.env_name, device=device)

    # ================= [Curriculum Learning è®¾ç½®] =================
    # å®šä¹‰å—åŠ›æƒ©ç½šçš„â€œè¯¾ç¨‹è¡¨â€
    # åˆå§‹æƒé‡ (Phase 1): å¾ˆå°ï¼Œè®©å®ƒå…ˆå­¦ä¼šè·‘
    W_FORCE_START = 1
    # [cite_start]æœ€ç»ˆæƒé‡ (Phase 2): è¾ƒå¤§ï¼Œè¿«ä½¿å®ƒä¼˜åŒ–å—åŠ›
    W_FORCE_END = 1
    # è¯¾ç¨‹å¼€å§‹çš„å¸§æ•° (å‰20%çš„æ—¶é—´å…ˆä¸åŠ å‹)
    CURRICULUM_START_FRAME = 0 
    # è¯¾ç¨‹ç»“æŸçš„å¸§æ•° (åœ¨è®­ç»ƒç»“æŸå‰è¾¾åˆ°æœ€å¤§æƒé‡)
    CURRICULUM_END_FRAME = cfg.collector.total_frames * 0.8 

    # åˆ›å»ºè·¨è¿›ç¨‹å…±äº«å˜é‡ (ç±»å‹ 'd' ä»£è¡¨ double/float)
    # è¿™ä¸ªå˜é‡çš„å€¼å¯ä»¥åœ¨ä¸»è¿›ç¨‹ä¿®æ”¹ï¼Œæ‰€æœ‰å­è¿›ç¨‹ç¯å¢ƒä¼šè‡ªåŠ¨è¯»å–åˆ°æ–°å€¼
    # shared_w_force = mp.Value('d', W_FORCE_START)
    manager = mp.Manager()
    shared_w_force = manager.Value('d', W_FORCE_START)
    # ==============================================================

    # å›ºå®šç»Ÿè®¡é‡å®šä¹‰
    FIXED_MEAN = [3.267685660834517, -0.3385894488464295, 1.9635006395349606, -0.1718040936161826, -0.12279842830682108, 0.007645809435244238, 0.011146266809907063, -0.0359171949460023, 0.02129872767178046, 339.37157534109247, 8.429046335233613, 0.0]
    FIXED_VAR = [10.854110464128235, 1.1635751967064711, 4.684817522924974, 0.9177030245326943, 0.16467684585450817, 0.1034575355366116, 0.027449087149541657, 0.010220331556196624, 0.016829017219362707, 500327.52070539613, 1705366.736736945, 0.0]
    # FIXED_MEAN = None
    # FIXED_VAR = None
    # ================= ä¿®æ”¹ç‚¹1ï¼šä½¿ç”¨ functools.partial + ParallelEnv åˆ›å»º Collector =================
    # ä½¿ç”¨ partial å›ºå®šå‚æ•°ï¼Œç¡®ä¿ Windows ä¸‹å¯ä»¥è¢« pickle åºåˆ—åŒ–
    # å»ºè®®å­ç¯å¢ƒä½¿ç”¨ "cpu"ï¼Œé¿å…å¤šè¿›ç¨‹ç«äº‰ GPU èµ„æºï¼Œæ•°æ®ä¼šç”± Collector ç»Ÿä¸€è½¬åˆ° device
    train_env_factory = functools.partial(
        make_train_env_wrapper,
        env_name=cfg.env.env_name,
        device="cpu", 
        fixed_mean=FIXED_MEAN,
        fixed_var=FIXED_VAR,
        shared_w_force=shared_w_force
    )

    collector = Collector(
        create_env_fn=lambda: ParallelEnv(
            num_workers=num_envs,
            create_env_fn=train_env_factory,
            serial_for_single=True, # å¦‚æœ num_envs=1 è‡ªåŠ¨åˆ‡å›ä¸²è¡Œ
        ),
        # create_env_fn=lambda: make_env(  # ç”¨lambdaå°è£…ï¼Œä¼ é€’VecNormçŠ¶æ€å‚æ•°
        #     cfg.env.env_name,
        #     device,
        #     vecnorm_frozen=False,  # è®­ç»ƒç¯å¢ƒï¼šä¸å†»ç»“VecNormï¼Œç»Ÿè®¡é‡éšè®­ç»ƒæ›´æ–°
        #     vecnorm_mean=FIXED_MEAN,
        #     vecnorm_var=FIXED_VAR
        # ),
        policy=actor,
        frames_per_batch=cfg.collector.frames_per_batch,
        total_frames=cfg.collector.total_frames,
        device=device,
        max_frames_per_traj=-1,
        compile_policy={"mode": compile_mode, "warmup": 1} if compile_mode else False,
        cudagraph_policy={"warmup": 10} if cfg.compile.cudagraphs else False,
    )

    # Create data buffer
    sampler = SamplerWithoutReplacement()
    data_buffer = TensorDictReplayBuffer(
        storage=LazyTensorStorage(
            cfg.collector.frames_per_batch,
            compilable=cfg.compile.compile,
            device=device,
        ),
        sampler=sampler,
        batch_size=cfg.loss.mini_batch_size,
        compilable=cfg.compile.compile,
    )

    # Create loss and adv modules
    adv_module = GAE(
        gamma=cfg.loss.gamma,
        lmbda=cfg.loss.gae_lambda,
        value_network=critic,
        average_gae=False,
        device=device,
        vectorized=not cfg.compile.compile,
    )

    loss_module = ClipPPOLoss(
        actor_network=actor,
        critic_network=critic,
        clip_epsilon=cfg.loss.clip_epsilon,
        loss_critic_type=cfg.loss.loss_critic_type,
        entropy_coeff=cfg.loss.entropy_coeff,
        critic_coeff=cfg.loss.critic_coeff,
        normalize_advantage=True,
    )

    # Create optimizers
    actor_optim = torch.optim.Adam(
        actor.parameters(), lr=torch.tensor(cfg.optim.lr, device=device), eps=1e-5
    )
    critic_optim = torch.optim.Adam(
        critic.parameters(), lr=torch.tensor(cfg.optim.lr, device=device), eps=1e-5
    )
    optim = group_optimizers(actor_optim, critic_optim)
    del actor_optim, critic_optim

    # Create logger
    logger = None
    if cfg.logger.backend:
        exp_name = generate_exp_name("PPO", f"{cfg.logger.exp_name}_{cfg.env.env_name}")
        logger = get_logger(
            cfg.logger.backend,
            logger_name="ppo",
            experiment_name=exp_name,
            swanlab_kwargs={
                "config": cfg,
                "group": cfg.logger.group_name,
                "project": cfg.logger.project_name
                or f"swanlab_{cfg.env.scenario_name}",
            },
        )
        logger_video = False
    else:
        logger_video = False

    # Create test environmentï¼šä¿æŒä¸²è¡Œå³å¯ï¼Œä½¿ç”¨å›ºå®šç»Ÿè®¡é‡
    test_env = make_env(
        cfg.env.env_name,
        device,
        from_pixels=logger_video,
        render_mode=None,
        enable_visualization=False,
        vecnorm_frozen=True,
        vecnorm_mean=FIXED_MEAN,
        vecnorm_var=FIXED_VAR
    )
    test_env.eval()

    # Updateå‡½æ•°
    def update(batch, num_network_updates):
        optim.zero_grad(set_to_none=True)
        alpha = torch.ones((), device=device)
        if cfg.optim.anneal_lr:
            alpha = 1 - (num_network_updates / total_network_updates)
            for group in optim.param_groups:
                group["lr"] = cfg.optim.lr * alpha
        if cfg.loss.anneal_clip_epsilon:
            loss_module.clip_epsilon.copy_(cfg.loss.clip_epsilon * alpha)
        num_network_updates = num_network_updates + 1

        loss = loss_module(batch)
        critic_loss = loss["loss_critic"]
        actor_loss = loss["loss_objective"] + loss["loss_entropy"]
        total_loss = critic_loss + actor_loss

        total_loss.backward()
        optim.step()
        return loss.detach().set("alpha", alpha), num_network_updates

    if cfg.compile.compile:
        update = compile_with_warmup(update, mode=compile_mode, warmup=1)
        adv_module = compile_with_warmup(adv_module, mode=compile_mode, warmup=1)

    if cfg.compile.cudagraphs:
        warnings.warn(
            "CudaGraphModule is experimental and may lead to silently wrong results. Use with caution.",
            category=UserWarning,
        )
        update = CudaGraphModule(update, in_keys=[], out_keys=[], warmup=5)
        adv_module = CudaGraphModule(adv_module)

    # Main loop
    collected_frames = 0
    num_network_updates = torch.zeros((), dtype=torch.int64, device=device)
    pbar = tqdm.tqdm(
        total=cfg.collector.total_frames,
        desc="Training",
        leave=True,
        dynamic_ncols=True
    )

    eval_round_counter = 0

    cfg_loss_ppo_epochs = cfg.loss.ppo_epochs
    cfg_optim_lr = torch.tensor(cfg.optim.lr, device=device)
    cfg_loss_clip_epsilon = cfg.loss.clip_epsilon
    cfg_loss_anneal_clip_eps = cfg.loss.anneal_clip_epsilon # ä¿®æ­£å˜é‡åå¼•ç”¨
    cfg_logger_test_interval = cfg.logger.test_interval
    cfg_logger_num_test_episodes = cfg.logger.num_test_episodes
    losses = TensorDict(batch_size=[cfg_loss_ppo_epochs, num_mini_batches])

    # ===================== ä¿®æ”¹ç‚¹2ï¼šç®€åŒ– Checkpoint (æ— éœ€æå–ç¯å¢ƒç»Ÿè®¡é‡) =====================
    def save_checkpoint(current_frames):
        # """å°è£…Checkpointä¿å­˜é€»è¾‘ï¼Œé€‚é…cfgé…ç½®ï¼Œæ–°å¢VecNormç»Ÿè®¡é‡ä¿å­˜"""
        # # å…³é”®ï¼šæå–è®­ç»ƒç¯å¢ƒçš„åŸå§‹TwoCarrierEnvå®ä¾‹ï¼Œè·å–VecNormç»Ÿè®¡é‡
        # raw_train_env = None
        # vecnorm_mean = np.zeros(12, dtype=np.float64)
        # vecnorm_var = np.ones(12, dtype=np.float64) * 1e-4  # ä¸ç¯å¢ƒé»˜è®¤æœ€å°æ–¹å·®ä¸€è‡´
        # vecnorm_frozen = False

        # try:
        #     # è§£åŒ…torchrl Collectorçš„ç¯å¢ƒå®ä¾‹ï¼Œè·å–åŸå§‹TwoCarrierEnv
        #     train_env_instance = collector.env
        #     raw_train_env = train_env_instance.unwrapped
        #     while not isinstance(raw_train_env, TwoCarrierEnv) and raw_train_env is not None:
        #         raw_train_env = getattr(raw_train_env, "_env", raw_train_env.unwrapped)
            
        #     if raw_train_env is not None:
        #         # æå–VecNormç»Ÿè®¡é‡
        #         vecnorm_mean = raw_train_env.vecnorm_mean.copy()
        #         vecnorm_var = raw_train_env.vecnorm_var.copy()
        #         vecnorm_frozen = raw_train_env.vecnorm_frozen
        # except Exception as e:
        #     print(f"âš ï¸ è·å–è®­ç»ƒç¯å¢ƒVecNormç»Ÿè®¡é‡å¤±è´¥ï¼ˆä¸å½±å“æ¨¡å‹ä¿å­˜ï¼‰ï¼š{e}")

        # # æ„é€ Checkpointå­—å…¸ï¼Œæ–°å¢VecNormç›¸å…³å†…å®¹
        # ckpt_dict = {
        #     "actor_state_dict": actor.state_dict(),
        #     "critic_state_dict": critic.state_dict(),
        #     "optim_state_dict": optim.state_dict(),
        #     "cfg": cfg,
        #     "collected_frames": current_frames,
        #     # æ–°å¢ï¼šVecNormç»Ÿè®¡é‡ï¼Œç”¨äºåç»­åŠ è½½æ—¶æ¢å¤å½’ä¸€åŒ–åˆ†å¸ƒ
        #     "vecnorm_mean": vecnorm_mean,
        #     "vecnorm_var": vecnorm_var,
        #     "vecnorm_frozen": vecnorm_frozen,
        # }
        # save_dir = cfg.checkpoint.checkpoint_dir
        # os.makedirs(save_dir, exist_ok=True)
        # save_path = os.path.join(save_dir, f"checkpoint_{current_frames}_frames.pt")
        # torch.save(ckpt_dict, save_path)
        # print(f"\nâœ… Checkpoint saved to: {save_path}")

        """ä»…ä¿å­˜æ¨¡å‹å’Œé…ç½®ï¼ŒVecNormä½¿ç”¨ä»£ç ä¸­å›ºå®šçš„å€¼"""
        ckpt_dict = {
            "actor_state_dict": actor.state_dict(),
            "critic_state_dict": critic.state_dict(),
            "optim_state_dict": optim.state_dict(),
            "cfg": cfg,
            "collected_frames": current_frames,
            # ç›´æ¥ä¿å­˜å›ºå®šçš„ç»Ÿè®¡é‡ï¼Œæˆ–è€…ä¿å­˜ Noneï¼Œå–å†³äºåç»­åŠ è½½éœ€æ±‚
            "vecnorm_mean": np.array(FIXED_MEAN),
            "vecnorm_var": np.array(FIXED_VAR),
            "vecnorm_frozen": True,
        }
        save_dir = cfg.checkpoint.checkpoint_dir
        os.makedirs(save_dir, exist_ok=True)
        save_path = os.path.join(save_dir, f"checkpoint_{current_frames}_frames.pt")
        torch.save(ckpt_dict, save_path)
        print(f"\nâœ… Checkpoint saved to: {save_path}")

    # load_checkpoint å‡½æ•° (ä¿æŒä¸å˜ï¼Œæ— éœ€ä¿®æ”¹)
    def load_checkpoint(ckpt_path, target_env=None):
        if not os.path.exists(ckpt_path):
            raise FileNotFoundError(f"Checkpointæ–‡ä»¶ä¸å­˜åœ¨ï¼š{ckpt_path}")
        ckpt_dict = torch.load(ckpt_path, map_location=device)
        actor.load_state_dict(ckpt_dict["actor_state_dict"])
        critic.load_state_dict(ckpt_dict["critic_state_dict"])
        optim.load_state_dict(ckpt_dict["optim_state_dict"])
        if target_env is not None:
            try:
                raw_env = target_env.unwrapped
                while not isinstance(raw_env, TwoCarrierEnv) and raw_env is not None:
                    raw_env = getattr(raw_env, "_env", raw_env.unwrapped)
                if raw_env is not None:
                    raw_env.vecnorm_mean = np.asarray(ckpt_dict["vecnorm_mean"], dtype=np.float64)
                    raw_env.vecnorm_var = np.asarray(ckpt_dict["vecnorm_var"], dtype=np.float64)
                    raw_env.vecnorm_frozen = ckpt_dict["vecnorm_frozen"]
            except Exception as e:
                print(f"âš ï¸ æ¢å¤VecNormç»Ÿè®¡é‡å¤±è´¥ï¼š{e}")
        return ckpt_dict["cfg"], ckpt_dict["collected_frames"]

    save_interval = cfg.checkpoint.save_interval
    last_saved_frames = 0

    collector_iter = iter(collector)
    total_iter = len(collector)
    
    for i in range(total_iter):
        timeit.printevery(1000, total_iter, erase=True)

        with timeit("collecting"):
            data = next(collector_iter)

        metrics_to_log = {}
        frames_in_batch = data.numel()
        collected_frames += frames_in_batch
        pbar.update(frames_in_batch)

        if save_interval > 0:
            if collected_frames // save_interval > last_saved_frames // save_interval:
                save_checkpoint(collected_frames)
                last_saved_frames = collected_frames

        episode_rewards = data["next", "episode_reward"][data["next", "done"]]
        if len(episode_rewards) > 0:
            episode_length = data["next", "step_count"][data["next", "done"]]
            metrics_to_log.update(
                {
                    "train/reward": episode_rewards.mean().item(),
                    "train/episode_length": episode_length.sum().item() / len(episode_length),
                }
            )

        # æå–å¹¶ç»Ÿè®¡ Reward åˆ†é¡¹
        try:
            reward_keys = [
                "reward_r_force", "reward_r_align_rear", "reward_r_align_front",
                # "reward_r_smooth", 
                "reward_r_progress", "reward_r_stability",
                "reward_val_force", "reward_val_delta_psi_rear", "reward_val_delta_psi_front"
            ]
            next_td = data["next"]
            for key in reward_keys:
                if key in next_td.keys():
                    val_mean = next_td[key].float().mean().item()
                    metrics_to_log[f"reward_parts/{key}"] = val_mean
                elif ("info", key) in next_td.keys(include_nested=True):
                    val_mean = next_td["info", key].float().mean().item()
                    metrics_to_log[f"reward_parts/{key}"] = val_mean
        except Exception as e:
            if i == 0: print(f"âš ï¸ æå– Reward Details å¤±è´¥: {e}")

        # --- æ ¸å¿ƒï¼šè®¡ç®—å¹¶æ›´æ–°å½“å‰æƒé‡ ---
        # 1. è®¡ç®—å½“å‰è¿›åº¦ (0.0 ~ 1.0)
        progress = (collected_frames - CURRICULUM_START_FRAME) / (CURRICULUM_END_FRAME - CURRICULUM_START_FRAME)
        progress = np.clip(progress, 0.0, 1.0) # é™åˆ¶åœ¨ 0~1 ä¹‹é—´

        # 2. çº¿æ€§æ’å€¼è®¡ç®—å½“å‰æƒé‡
        current_w_force = W_FORCE_START + (W_FORCE_END - W_FORCE_START) * progress
        
        # 3. æ›´æ–°å…±äº«å˜é‡ (æ‰€æœ‰å¹¶è¡Œç¯å¢ƒä¼šç«‹å³ç”Ÿæ•ˆ)
        shared_w_force.value = current_w_force

        # 4. è®°å½•åˆ° TensorBoard
        if i % 10 == 0:
            print(f"Frame {collected_frames}: w_force updated to {current_w_force:.5f}")
        metrics_to_log["train/w_force"] = current_w_force  # è®°å½•åˆ° wandb/tensorboard æ–¹ä¾¿è§‚å¯Ÿ
        # ------------------------------------

        with timeit("training"):
            for j in range(cfg_loss_ppo_epochs):
                with torch.no_grad(), timeit("adv"):
                    torch.compiler.cudagraph_mark_step_begin()
                    data = adv_module(data)
                    if compile_mode:
                        data = data.clone()

                with timeit("rb - extend"):
                    data_reshape = data.reshape(-1)
                    data_buffer.extend(data_reshape)

                for k, batch in enumerate(data_buffer):
                    with timeit("update"):
                        torch.compiler.cudagraph_mark_step_begin()
                        loss, num_network_updates = update(
                            batch, num_network_updates=num_network_updates
                        )
                        loss = loss.clone()
                    num_network_updates = num_network_updates.clone()
                    losses[j, k] = loss.select(
                        "loss_critic", "loss_entropy", "loss_objective"
                    )

        losses_mean = losses.apply(lambda x: x.float().mean(), batch_size=[])
        for key, value in losses_mean.items():
            metrics_to_log.update({f"train/{key}": value.item()})
        metrics_to_log.update(
            {
                "train/lr": loss["alpha"] * cfg_optim_lr,
                "train/clip_epsilon": loss["alpha"] * cfg_loss_clip_epsilon
                if cfg_loss_anneal_clip_eps
                else cfg_loss_clip_epsilon,
            }
        )

        # ================= ä¿®æ”¹ç‚¹3ï¼šç®€åŒ–è¯„æµ‹ (æ— éœ€åŒæ­¥ VecNorm) =================
        with torch.no_grad(), set_exploration_type(ExplorationType.DETERMINISTIC), timeit("eval"):
            if ((i - 1) * frames_in_batch) // cfg_logger_test_interval < (
                i * frames_in_batch
            ) // cfg_logger_test_interval:
                eval_round_counter += 1
                actor.eval()
                print(f"\n============= å¼€å§‹ç¬¬ {eval_round_counter} è½®è¯„æµ‹ =============")
                # train_vecnorm_mean = np.zeros(12, dtype=np.float64)
                # train_vecnorm_var = np.ones(12, dtype=np.float64) * 1e-4
                # try:
                #     # è§£åŒ…è®­ç»ƒç¯å¢ƒï¼Œè·å–æœ€æ–°çš„mean/varï¼ˆå¤ç”¨ä½ ç°æœ‰Checkpointä¸­çš„æå–é€»è¾‘ï¼‰
                #     raw_train_env = collector.env.unwrapped
                #     while not isinstance(raw_train_env, TwoCarrierEnv) and raw_train_env is not None:
                #         raw_train_env = getattr(raw_train_env, "_env", raw_train_env.unwrapped)
                    
                #     if raw_train_env is not None:
                #         train_vecnorm_mean = raw_train_env.vecnorm_mean.copy()
                #         train_vecnorm_var = raw_train_env.vecnorm_var.copy()
                #         # print(f"âœ… æå–åˆ°è®­ç»ƒç¯å¢ƒæœ€æ–°VecNormï¼šå‡å€¼å‰5ç»´ {train_vecnorm_mean[:5].round(6)}ï¼Œæ–¹å·®å‰5ç»´ {train_vecnorm_var[:5].round(6)}")
                # except Exception as e:
                #     print(f"âš ï¸ æå–è®­ç»ƒç¯å¢ƒVecNormå¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼ï¼š{e}")

                # # ===== æ–°å¢ï¼šæ­¥éª¤2 - åŒæ­¥åˆ°test_envï¼Œå¹¶ç¡®ä¿å†»ç»“ =====
                # try:
                #     raw_test_env = test_env.unwrapped
                #     while not isinstance(raw_test_env, TwoCarrierEnv) and raw_test_env is not None:
                #         raw_test_env = getattr(raw_test_env, "_env", raw_test_env.unwrapped)
                    
                #     if raw_test_env is not None:
                #         # è¦†ç›–test_envçš„åˆå§‹mean/varä¸ºè®­ç»ƒç¯å¢ƒçš„æœ€æ–°å€¼
                #         raw_test_env.vecnorm_mean = train_vecnorm_mean
                #         raw_test_env.vecnorm_var = train_vecnorm_var
                #         # å¼ºåˆ¶ç¡®è®¤å†»ç»“ï¼Œé¿å…æ„å¤–æ›´æ–°
                #         raw_test_env.vecnorm_frozen = True
                #         # print(f"âœ… å·²å°†è®­ç»ƒç¯å¢ƒVecNormåŒæ­¥åˆ°test_envï¼Œä¸”ä¿æŒå†»ç»“çŠ¶æ€")
                # except Exception as e:
                #     print(f"âš ï¸ åŒæ­¥VecNormåˆ°test_envå¤±è´¥ï¼š{e}")

                # Test Env å·²ç»åˆå§‹åŒ–ä¸º FIXED_MEAN/VAR ä¸” Frozenï¼Œç›´æ¥è·‘å³å¯
                test_rewards = eval_model(
                    actor, test_env, num_episodes=cfg_logger_num_test_episodes, eval_round=eval_round_counter
                )
                
                metrics_to_log.update(
                    {
                        "eval/reward": test_rewards.item() if isinstance(test_rewards, torch.Tensor) else test_rewards,
                        "eval/round": eval_round_counter,
                    }
                )
                
                actor.train()
                print(f"============= ç¬¬ {eval_round_counter} è½®è¯„æµ‹ç»“æŸ =============")

        if logger:
            metrics_to_log.update(timeit.todict(prefix="time"))
            metrics_to_log["time/speed"] = pbar.format_dict["rate"]
            for key, value in metrics_to_log.items():
                logger.log_scalar(key, value, collected_frames)

        collector.update_policy_weights_()

    if save_interval > 0:
        save_checkpoint(cfg.collector.total_frames)
    
    collector.shutdown()
    if not test_env.is_closed:
        test_env.close()
    pbar.close()
    print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼Œæ‰€æœ‰Checkpointå·²ä¿å­˜ï¼")

# ================= ä¿®æ”¹ç‚¹4ï¼šWindowså¤šè¿›ç¨‹å…¥å£ä¿æŠ¤ =================
if __name__ == "__main__":
    # å¼ºåˆ¶ä½¿ç”¨ spawnï¼Œè§£å†³ Windows pickling é—®é¢˜åŠ CUDA å…¼å®¹æ€§
    try:
        mp.set_start_method("spawn", force=True)
    except RuntimeError:
        pass
    main()