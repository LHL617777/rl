# æ¨¡å‹è®­ç»ƒè„šæœ¬ï¼šPPOç®—æ³•åœ¨OCCTç¯å¢ƒä¸­çš„å®ç°
"""
This script reproduces the Proximal Policy Optimization (PPO) Algorithm.
é€‚é…ï¼šç¯å¢ƒå†…éƒ¨è‡ªå®šä¹‰VecNormè§‚æµ‹å½’ä¸€åŒ–ï¼Œç§»é™¤åŒé‡å½’ä¸€åŒ–ï¼Œå¢å¼ºCheckpointå…¼å®¹æ€§
å¹¶è¡Œç‰ˆï¼šæ”¯æŒ Windows/Linuxï¼Œå›ºå®š VecNorm ç»Ÿè®¡é‡
"""
from __future__ import annotations
import sys
import os
import numpy as np
import functools # æ–°å¢ï¼šç”¨äºæ„é€ å¯åºåˆ—åŒ–çš„ç¯å¢ƒå‡½æ•°
import torch.multiprocessing as mp # æ–°å¢ï¼šå¤šè¿›ç¨‹ç®¡ç†

# æ·»åŠ æœ¬åœ°è‡ªå®šä¹‰torchrlçš„æ ¹ç›®å½•ï¼ˆE:\rl\torchrl\çš„ä¸Šä¸€çº§ç›®å½•ï¼Œå³E:\rl\ï¼‰
sys.path.insert(0, "E:\\rl")  # insert(0)è¡¨ç¤ºå°†è¯¥è·¯å¾„æ”¾åœ¨æœç´¢ä¼˜å…ˆçº§ç¬¬1ä½
# sys.path.insert(0, "/home/yons/Graduation/rl")  # Linuxç³»ç»Ÿä¸­çš„è·¯å¾„
# TIspdizNBNWYgfoUxNl86    swanlab API Key

import warnings

import hydra
import torchrl
from torchrl._utils import compile_with_warmup
from torchrl.record.loggers.swanlab import SwanLabLogger
import gymnasium as gym
from occt_2d2c import TwoCarrierEnv
from omegaconf import DictConfig
from torchrl.collectors import Collector
from torchrl.envs import ParallelEnv # æ–°å¢ï¼šå¹¶è¡Œç¯å¢ƒåŒ…è£…å™¨

# å¯¼å…¥å·¥å…·å‡½æ•°
from utils_ppo_occt import eval_model, make_env, make_ppo_models


# =========================================================================
# å…³é”®ä¿®æ”¹ï¼šå®šä¹‰æ¨¡å—çº§è¾…åŠ©å‡½æ•° (å¿…é¡»åœ¨ main ä¹‹å¤–ï¼Œå¦åˆ™ Windows æŠ¥é”™)
# =========================================================================
def make_train_env_wrapper(env_name, device, shared_w_force):
    """
    ç”¨äºåœ¨å­è¿›ç¨‹ä¸­åˆ›å»ºç¯å¢ƒçš„åŒ…è£…å‡½æ•°ã€‚
    """
    return make_env(
        env_name,
        device=device,  # å­è¿›ç¨‹é€šå¸¸å»ºè®®ç”¨ CPUï¼Œç”± Collector ç»Ÿä¸€ä¼ åˆ° GPU
        shared_w_force=shared_w_force
    )


@hydra.main(config_path="", config_name="config_occt", version_base="1.1")
def main(cfg: DictConfig):

    import torch.optim
    import tqdm

    from tensordict import TensorDict
    from tensordict.nn import CudaGraphModule

    from torchrl._utils import timeit
    from torchrl.data import LazyTensorStorage, TensorDictReplayBuffer
    from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement
    from torchrl.envs import ExplorationType, set_exploration_type
    from torchrl.objectives import ClipPPOLoss, group_optimizers
    from torchrl.objectives.value.advantages import GAE
    from torchrl.record.loggers import generate_exp_name, get_logger

    torch.set_float32_matmul_precision("high")

    device = cfg.optim.device
    if device in ("", None):
        if torch.cuda.is_available():
            device = "cuda:0"
        else:
            device = "cpu"
    device = torch.device(device)

    # å¹¶è¡Œç¯å¢ƒæ•°é‡é…ç½® (ä»cfgè¯»å–ï¼Œé»˜è®¤ä¸º4)
    num_envs = getattr(cfg.env, "num_envs", 4)
    # num_envs = 1
    print(f"ğŸš€ å¯åŠ¨å¹¶è¡Œè®­ç»ƒ | å¹¶è¡Œç¯å¢ƒæ•°: {num_envs} | å¹³å°: {os.name}")

    num_mini_batches = cfg.collector.frames_per_batch // cfg.loss.mini_batch_size
    total_network_updates = (
        (cfg.collector.total_frames // cfg.collector.frames_per_batch)
        * cfg.loss.ppo_epochs
        * num_mini_batches
    )

    compile_mode = None
    if cfg.compile.compile:
        compile_mode = cfg.compile.compile_mode
        if compile_mode in ("", None):
            if cfg.compile.cudagraphs:
                compile_mode = "default"
            else:
                compile_mode = "reduce-overhead"

    # Create models
    actor, critic = make_ppo_models(cfg.env.env_name, device=device)

    # ================= [Curriculum Learning è®¾ç½®] =================
    # å®šä¹‰å—åŠ›æƒ©ç½šçš„â€œè¯¾ç¨‹è¡¨â€
    # åˆå§‹æƒé‡ (Phase 1): å¾ˆå°ï¼Œè®©å®ƒå…ˆå­¦ä¼šè·‘
    W_FORCE_START = 1
    # [cite_start]æœ€ç»ˆæƒé‡ (Phase 2): è¾ƒå¤§ï¼Œè¿«ä½¿å®ƒä¼˜åŒ–å—åŠ›
    W_FORCE_END = 1
    # è¯¾ç¨‹å¼€å§‹çš„å¸§æ•° (å‰20%çš„æ—¶é—´å…ˆä¸åŠ å‹)
    CURRICULUM_START_FRAME = 0 
    # è¯¾ç¨‹ç»“æŸçš„å¸§æ•° (åœ¨è®­ç»ƒç»“æŸå‰è¾¾åˆ°æœ€å¤§æƒé‡)
    CURRICULUM_END_FRAME = cfg.collector.total_frames * 0.8 

    # åˆ›å»ºè·¨è¿›ç¨‹å…±äº«å˜é‡ (ç±»å‹ 'd' ä»£è¡¨ double/float)
    # è¿™ä¸ªå˜é‡çš„å€¼å¯ä»¥åœ¨ä¸»è¿›ç¨‹ä¿®æ”¹ï¼Œæ‰€æœ‰å­è¿›ç¨‹ç¯å¢ƒä¼šè‡ªåŠ¨è¯»å–åˆ°æ–°å€¼
    # shared_w_force = mp.Value('d', W_FORCE_START)
    manager = mp.Manager()
    shared_w_force = manager.Value('d', W_FORCE_START)
    # ==============================================================

    # # å›ºå®šç»Ÿè®¡é‡å®šä¹‰
    # FIXED_MEAN = [3.267685660834517, -0.3385894488464295, 1.9635006395349606, -0.1718040936161826, -0.12279842830682108, 0.007645809435244238, 0.011146266809907063, -0.0359171949460023, 0.02129872767178046, 339.37157534109247, 8.429046335233613, 0.0]
    # FIXED_VAR = [10.854110464128235, 1.1635751967064711, 4.684817522924974, 0.9177030245326943, 0.16467684585450817, 0.1034575355366116, 0.027449087149541657, 0.010220331556196624, 0.016829017219362707, 500327.52070539613, 1705366.736736945, 0.0]
    # FIXED_MEAN = None
    # FIXED_VAR = None
    # ================= ä¿®æ”¹ç‚¹1ï¼šä½¿ç”¨ functools.partial + ParallelEnv åˆ›å»º Collector =================
    # ä½¿ç”¨ partial å›ºå®šå‚æ•°ï¼Œç¡®ä¿ Windows ä¸‹å¯ä»¥è¢« pickle åºåˆ—åŒ–
    # å»ºè®®å­ç¯å¢ƒä½¿ç”¨ "cpu"ï¼Œé¿å…å¤šè¿›ç¨‹ç«äº‰ GPU èµ„æºï¼Œæ•°æ®ä¼šç”± Collector ç»Ÿä¸€è½¬åˆ° device
    train_env_factory = functools.partial(
        make_train_env_wrapper,
        env_name=cfg.env.env_name,
        device="cpu", 
        # fixed_mean=FIXED_MEAN,
        # fixed_var=FIXED_VAR,
        shared_w_force=shared_w_force
    )

    collector = Collector(
        create_env_fn=lambda: ParallelEnv(
            num_workers=num_envs,
            create_env_fn=train_env_factory,
            serial_for_single=True, # å¦‚æœ num_envs=1 è‡ªåŠ¨åˆ‡å›ä¸²è¡Œ
        ),
        policy=actor,
        frames_per_batch=cfg.collector.frames_per_batch,
        total_frames=cfg.collector.total_frames,
        device=device,
        max_frames_per_traj=-1,
        compile_policy={"mode": compile_mode, "warmup": 1} if compile_mode else False,
        cudagraph_policy={"warmup": 10} if cfg.compile.cudagraphs else False,
    )

    # Create data buffer
    sampler = SamplerWithoutReplacement()
    data_buffer = TensorDictReplayBuffer(
        storage=LazyTensorStorage(
            cfg.collector.frames_per_batch,
            compilable=cfg.compile.compile,
            device=device,
        ),
        sampler=sampler,
        batch_size=cfg.loss.mini_batch_size,
        compilable=cfg.compile.compile,
    )

    # Create loss and adv modules
    adv_module = GAE(
        gamma=cfg.loss.gamma,
        lmbda=cfg.loss.gae_lambda,
        value_network=critic,
        average_gae=False,
        device=device,
        vectorized=not cfg.compile.compile,
    )

    loss_module = ClipPPOLoss(
        actor_network=actor,
        critic_network=critic,
        clip_epsilon=cfg.loss.clip_epsilon,
        loss_critic_type=cfg.loss.loss_critic_type,
        entropy_coeff=cfg.loss.entropy_coeff,
        critic_coeff=cfg.loss.critic_coeff,
        normalize_advantage=True,
    )

    # Create optimizers
    actor_optim = torch.optim.Adam(
        actor.parameters(), lr=torch.tensor(cfg.optim.lr, device=device), eps=1e-5
    )
    critic_optim = torch.optim.Adam(
        critic.parameters(), lr=torch.tensor(cfg.optim.lr, device=device), eps=1e-5
    )
    optim = group_optimizers(actor_optim, critic_optim)
    del actor_optim, critic_optim

    def save_checkpoint(current_frames):
        """ä»…ä¿å­˜æ¨¡å‹å’Œé…ç½®å€¼"""
        ckpt_dict = {
            "actor_state_dict": actor.state_dict(),
            "critic_state_dict": critic.state_dict(),
            "optim_state_dict": optim.state_dict(),
            "cfg": cfg,
            "collected_frames": current_frames
        }
        save_dir = cfg.checkpoint.checkpoint_dir
        os.makedirs(save_dir, exist_ok=True)
        save_path = os.path.join(save_dir, f"checkpoint_{current_frames}_frames.pt")
        torch.save(ckpt_dict, save_path)
        print(f"\nâœ… Checkpoint saved to: {save_path}")

    # load_checkpoint å‡½æ•° (ä¿æŒä¸å˜ï¼Œæ— éœ€ä¿®æ”¹)
    def load_checkpoint(ckpt_path, target_env=None):
        if not os.path.exists(ckpt_path):
            raise FileNotFoundError(f"Checkpointæ–‡ä»¶ä¸å­˜åœ¨ï¼š{ckpt_path}")
        ckpt_dict = torch.load(ckpt_path, map_location=device, weights_only=False)
        actor.load_state_dict(ckpt_dict["actor_state_dict"])
        critic.load_state_dict(ckpt_dict["critic_state_dict"])
        optim.load_state_dict(ckpt_dict["optim_state_dict"])
        return ckpt_dict["cfg"], ckpt_dict["collected_frames"]
    
    # ================= [æ–°å¢] æ–­ç‚¹é‡è®­/é¢„è®­ç»ƒåŠ è½½é€»è¾‘ =================
    # å‡è®¾ä½ åœ¨ config_occt.yaml ä¸­æ·»åŠ äº†å­—æ®µ: checkpoint.load_path (é»˜è®¤ä¸º null)
    # æˆ–è€…ç›´æ¥åœ¨è¿™é‡Œç¡¬ç¼–ç æµ‹è¯•
    load_path = cfg.checkpoint.get("load_path", None)  
    # ç¤ºä¾‹ï¼š load_path = "checkpoints/checkpoint_1000000_frames.pt" 
    
    if load_path and os.path.exists(load_path):
        print(f"\nğŸ”„ æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {load_path}")
        try:
            # å¤ç”¨ä½ å·²æœ‰çš„ load_checkpoint å‡½æ•°ï¼Œä½†æˆ‘ä»¬éœ€è¦å¾®è°ƒä¸€ä¸‹è°ƒç”¨æ–¹å¼
            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬è¿˜æ²¡åˆ›å»º Collectorï¼Œæ‰€ä»¥æš‚æ—¶ä¼  None ç»™ target_env
            # å¦‚æœ VecNorm æ˜¯å›ºå®šçš„ (FIXED_MEAN)ï¼Œåˆ™ä¸éœ€è¦ä» checkpoint æ¢å¤ env ç»Ÿè®¡é‡
            loaded_cfg, loaded_frames = load_checkpoint(load_path, target_env=None)
            
            print(f"âœ… æ¨¡å‹æƒé‡å·²æ¢å¤ (åŸè®­ç»ƒæ­¥æ•°: {loaded_frames})")
            
            # ã€ç­–ç•¥é€‰æ‹©ã€‘
            # é€‰é¡¹ A: å½»åº•çš„ "Resume" (æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œç»§ç»­ä¹‹å‰çš„è®­ç»ƒæµ)
            # é€‚ç”¨äºï¼šæœºå™¨æ–­ç”µäº†ï¼Œæƒ³æ¥ç€è·‘
            # -------------------------------------------------------------
            # collected_frames = loaded_frames 
            # print("   -> æ¨¡å¼: Resume (ç»§æ‰¿å†å²æ­¥æ•°å’Œä¼˜åŒ–å™¨çŠ¶æ€)")

            # é€‰é¡¹ B: "Finetune" / è¯¾ç¨‹å­¦ä¹  (é‡ç½®ä¼˜åŒ–å™¨ï¼Œé‡ç½®æ­¥æ•°ï¼Œåªä¿ç•™ç½‘ç»œæƒé‡)
            # é€‚ç”¨äºï¼šä½ çš„åœºæ™¯ï¼ˆç¬¬ä¸€é˜¶æ®µç»“æŸï¼Œæ”¹å˜è¶…å‚å¦‚ max_stepsï¼Œè¿›å…¥ç¬¬äºŒé˜¶æ®µï¼‰
            # -------------------------------------------------------------
            # å¦‚æœæ˜¯è¯¾ç¨‹å­¦ä¹ ï¼Œæˆ‘ä»¬é€šå¸¸å¸Œæœ›å­¦ä¹ ç‡é‡æ–°å¼€å§‹è¡°å‡ï¼Œæˆ–è€…ä½¿ç”¨è¾ƒå°çš„æ’å®šå­¦ä¹ ç‡
            # æ‰€ä»¥æˆ‘ä»¬åªåŠ è½½ç½‘ç»œæƒé‡ï¼Œä¸åŠ è½½ optim_state_dict (é™¤éä½ æƒ³ä¿æŒåŠ¨é‡)
            # è¿™é‡Œæˆ‘åšä¸€ä¸ªæŠ˜ä¸­ï¼šåŠ è½½æ‰€æœ‰çŠ¶æ€ï¼Œä½†å…è®¸ä½ æ‰‹åŠ¨é‡ç½® collected_frames
            
            # å¦‚æœä½ æƒ³è®©è¿›åº¦æ¡ä» 0 å¼€å§‹ (é€‚åˆè¯¾ç¨‹å­¦ä¹ çš„ç¬¬äºŒé˜¶æ®µ):
            collected_frames = 0 
            print("   -> æ¨¡å¼: Curriculum Finetune (æ­¥æ•°é‡ç½®ä¸º0ï¼Œåœ¨æ–°é…ç½®ä¸‹ç»§ç»­è®­ç»ƒ)")
            
        except Exception as e:
            print(f"âŒ åŠ è½½ Checkpoint å¤±è´¥: {e}")
            raise e
    else:
        print("\nğŸ†• æœªæŒ‡å®š load_path æˆ–æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¼€å§‹ä»å¤´è®­ç»ƒ")
    # ====================================================================

    # Create logger
    logger = None
    if cfg.logger.backend:
        exp_name = generate_exp_name("PPO", f"{cfg.logger.exp_name}_{cfg.env.env_name}")
        logger = get_logger(
            cfg.logger.backend,
            logger_name="ppo",
            experiment_name=exp_name,
            swanlab_kwargs={
                "config": cfg,
                "group": cfg.logger.group_name,
                "project": cfg.logger.project_name
                or f"swanlab_{cfg.env.scenario_name}",
            },
        )
        logger_video = False
    else:
        logger_video = False

    # Create test environmentï¼šä¿æŒä¸²è¡Œå³å¯ï¼Œä½¿ç”¨å›ºå®šç»Ÿè®¡é‡
    test_env = make_env(
        cfg.env.env_name,
        device,
        from_pixels=logger_video,
        render_mode=None,
        enable_visualization=False,
        shared_w_force=shared_w_force
    )
    test_env.eval()

    # Updateå‡½æ•°
    def update(batch, num_network_updates):
        optim.zero_grad(set_to_none=True)
        alpha = torch.ones((), device=device)
        if cfg.optim.anneal_lr:
            alpha = 1 - (num_network_updates / total_network_updates)
            for group in optim.param_groups:
                group["lr"] = cfg.optim.lr * alpha
        if cfg.loss.anneal_clip_epsilon:
            loss_module.clip_epsilon.copy_(cfg.loss.clip_epsilon * alpha)
        num_network_updates = num_network_updates + 1

        loss = loss_module(batch)
        critic_loss = loss["loss_critic"]
        actor_loss = loss["loss_objective"] + loss["loss_entropy"]
        total_loss = critic_loss + actor_loss

        total_loss.backward()
        optim.step()
        return loss.detach().set("alpha", alpha), num_network_updates

    if cfg.compile.compile:
        update = compile_with_warmup(update, mode=compile_mode, warmup=1)
        adv_module = compile_with_warmup(adv_module, mode=compile_mode, warmup=1)

    if cfg.compile.cudagraphs:
        warnings.warn(
            "CudaGraphModule is experimental and may lead to silently wrong results. Use with caution.",
            category=UserWarning,
        )
        update = CudaGraphModule(update, in_keys=[], out_keys=[], warmup=5)
        adv_module = CudaGraphModule(adv_module)

    # Main loop
    collected_frames = 0
    num_network_updates = torch.zeros((), dtype=torch.int64, device=device)
    pbar = tqdm.tqdm(
        total=cfg.collector.total_frames,
        desc="Training",
        leave=True,
        dynamic_ncols=True
    )

    eval_round_counter = 0

    cfg_loss_ppo_epochs = cfg.loss.ppo_epochs
    cfg_optim_lr = torch.tensor(cfg.optim.lr, device=device)
    cfg_loss_clip_epsilon = cfg.loss.clip_epsilon
    cfg_loss_anneal_clip_eps = cfg.loss.anneal_clip_epsilon # ä¿®æ­£å˜é‡åå¼•ç”¨
    cfg_logger_test_interval = cfg.logger.test_interval
    cfg_logger_num_test_episodes = cfg.logger.num_test_episodes
    losses = TensorDict(batch_size=[cfg_loss_ppo_epochs, num_mini_batches])


    save_interval = cfg.checkpoint.save_interval
    last_saved_frames = 0

    collector_iter = iter(collector)
    total_iter = len(collector)
    
    for i in range(total_iter):
        timeit.printevery(1000, total_iter, erase=True)

        with timeit("collecting"):
            data = next(collector_iter)

        metrics_to_log = {}
        frames_in_batch = data.numel()
        collected_frames += frames_in_batch
        pbar.update(frames_in_batch)

        if save_interval > 0:
            if collected_frames // save_interval > last_saved_frames // save_interval:
                save_checkpoint(collected_frames)
                last_saved_frames = collected_frames

        episode_rewards = data["next", "episode_reward"][data["next", "done"]]
        if len(episode_rewards) > 0:
            episode_length = data["next", "step_count"][data["next", "done"]]
            metrics_to_log.update(
                {
                    "train/reward": episode_rewards.mean().item(),
                    "train/episode_length": episode_length.sum().item() / len(episode_length),
                }
            )

        # æå–å¹¶ç»Ÿè®¡ Reward åˆ†é¡¹
        try:
            reward_keys = [
                "reward_r_force", "reward_r_align_rear", "reward_r_align_front",
                # "reward_r_smooth", 
                "reward_r_progress", "reward_r_stability",
                "reward_val_force", "reward_val_delta_psi_rear", "reward_val_delta_psi_front"
            ]
            next_td = data["next"]
            for key in reward_keys:
                if key in next_td.keys():
                    val_mean = next_td[key].float().mean().item()
                    metrics_to_log[f"reward_parts/{key}"] = val_mean
                elif ("info", key) in next_td.keys(include_nested=True):
                    val_mean = next_td["info", key].float().mean().item()
                    metrics_to_log[f"reward_parts/{key}"] = val_mean
        except Exception as e:
            if i == 0: print(f"âš ï¸ æå– Reward Details å¤±è´¥: {e}")

        # --- æ ¸å¿ƒï¼šè®¡ç®—å¹¶æ›´æ–°å½“å‰æƒé‡ ---
        # 1. è®¡ç®—å½“å‰è¿›åº¦ (0.0 ~ 1.0)
        progress = (collected_frames - CURRICULUM_START_FRAME) / (CURRICULUM_END_FRAME - CURRICULUM_START_FRAME)
        progress = np.clip(progress, 0.0, 1.0) # é™åˆ¶åœ¨ 0~1 ä¹‹é—´

        # 2. çº¿æ€§æ’å€¼è®¡ç®—å½“å‰æƒé‡
        current_w_force = W_FORCE_START + (W_FORCE_END - W_FORCE_START) * progress
        
        # 3. æ›´æ–°å…±äº«å˜é‡ (æ‰€æœ‰å¹¶è¡Œç¯å¢ƒä¼šç«‹å³ç”Ÿæ•ˆ)
        shared_w_force.value = current_w_force

        # 4. è®°å½•åˆ° TensorBoard
        if i % 10 == 0:
            print(f"Frame {collected_frames}: w_force updated to {current_w_force:.5f}")
        metrics_to_log["train/w_force"] = current_w_force  # è®°å½•åˆ° wandb/tensorboard æ–¹ä¾¿è§‚å¯Ÿ
        # ------------------------------------

        with timeit("training"):
            for j in range(cfg_loss_ppo_epochs):
                with torch.no_grad(), timeit("adv"):
                    torch.compiler.cudagraph_mark_step_begin()
                    data = adv_module(data)
                    if compile_mode:
                        data = data.clone()

                with timeit("rb - extend"):
                    data_reshape = data.reshape(-1)
                    data_buffer.extend(data_reshape)

                for k, batch in enumerate(data_buffer):
                    with timeit("update"):
                        torch.compiler.cudagraph_mark_step_begin()
                        loss, num_network_updates = update(
                            batch, num_network_updates=num_network_updates
                        )
                        loss = loss.clone()
                    num_network_updates = num_network_updates.clone()
                    losses[j, k] = loss.select(
                        "loss_critic", "loss_entropy", "loss_objective"
                    )

        losses_mean = losses.apply(lambda x: x.float().mean(), batch_size=[])
        for key, value in losses_mean.items():
            metrics_to_log.update({f"train/{key}": value.item()})
        metrics_to_log.update(
            {
                "train/lr": loss["alpha"] * cfg_optim_lr,
                "train/clip_epsilon": loss["alpha"] * cfg_loss_clip_epsilon
                if cfg_loss_anneal_clip_eps
                else cfg_loss_clip_epsilon,
            }
        )

        # ================= ä¿®æ”¹ç‚¹3ï¼šç®€åŒ–è¯„æµ‹ (æ— éœ€åŒæ­¥ VecNorm) =================
        with torch.no_grad(), set_exploration_type(ExplorationType.DETERMINISTIC), timeit("eval"):
            if ((i - 1) * frames_in_batch) // cfg_logger_test_interval < (
                i * frames_in_batch
            ) // cfg_logger_test_interval:
                eval_round_counter += 1
                actor.eval()
                print(f"\n============= å¼€å§‹ç¬¬ {eval_round_counter} è½®è¯„æµ‹ =============")
                test_rewards = eval_model(
                    actor, test_env, num_episodes=cfg_logger_num_test_episodes, eval_round=eval_round_counter
                )
                
                metrics_to_log.update(
                    {
                        "eval/reward": test_rewards.item() if isinstance(test_rewards, torch.Tensor) else test_rewards,
                        "eval/round": eval_round_counter,
                    }
                )
                
                actor.train()
                print(f"============= ç¬¬ {eval_round_counter} è½®è¯„æµ‹ç»“æŸ =============")

        if logger:
            metrics_to_log.update(timeit.todict(prefix="time"))
            metrics_to_log["time/speed"] = pbar.format_dict["rate"]
            for key, value in metrics_to_log.items():
                logger.log_scalar(key, value, collected_frames)

        collector.update_policy_weights_()

    if save_interval > 0:
        save_checkpoint(cfg.collector.total_frames)
    
    collector.shutdown()
    if not test_env.is_closed:
        test_env.close()
    pbar.close()
    print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼Œæ‰€æœ‰Checkpointå·²ä¿å­˜ï¼")

# ================= ä¿®æ”¹ç‚¹4ï¼šWindowså¤šè¿›ç¨‹å…¥å£ä¿æŠ¤ =================
if __name__ == "__main__":
    # å¼ºåˆ¶ä½¿ç”¨ spawnï¼Œè§£å†³ Windows pickling é—®é¢˜åŠ CUDA å…¼å®¹æ€§
    try:
        mp.set_start_method("spawn", force=True)
    except RuntimeError:
        pass
    main()