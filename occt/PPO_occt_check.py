# æ¨¡å‹è®­ç»ƒè„šæœ¬ï¼šPPOç®—æ³•åœ¨OCCTç¯å¢ƒä¸­çš„å®ç°
"""
This script reproduces the Proximal Policy Optimization (PPO) Algorithm.
é€‚é…ï¼šç¯å¢ƒå†…éƒ¨è‡ªå®šä¹‰VecNormè§‚æµ‹å½’ä¸€åŒ–ï¼Œç§»é™¤åŒé‡å½’ä¸€åŒ–ï¼Œå¢å¼ºCheckpointå…¼å®¹æ€§
"""
from __future__ import annotations
import sys
import os
import numpy as np

# æ·»åŠ æœ¬åœ°è‡ªå®šä¹‰torchrlçš„æ ¹ç›®å½•ï¼ˆE:\rl\torchrl\çš„ä¸Šä¸€çº§ç›®å½•ï¼Œå³E:\rl\ï¼‰
sys.path.insert(0, "E:\\rl")  # insert(0)è¡¨ç¤ºå°†è¯¥è·¯å¾„æ”¾åœ¨æœç´¢ä¼˜å…ˆçº§ç¬¬1ä½
# sys.path.insert(0, "/home/yons/Graduation/rl")  # Linuxç³»ç»Ÿä¸­çš„è·¯å¾„
# TIspdizNBNWYgfoUxNl86    swanlab API Key

import warnings

import hydra
import torchrl
from torchrl._utils import compile_with_warmup
from torchrl.record.loggers.swanlab import SwanLabLogger
import gymnasium as gym
from occt_2d2c import TwoCarrierEnv
from omegaconf import DictConfig
from torchrl.collectors import Collector


@hydra.main(config_path="", config_name="config_occt", version_base="1.1")
def main(cfg: DictConfig):

    import torch.optim
    import tqdm

    from tensordict import TensorDict
    from tensordict.nn import CudaGraphModule

    from torchrl._utils import timeit
    from torchrl.data import LazyTensorStorage, TensorDictReplayBuffer
    from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement
    from torchrl.envs import ExplorationType, set_exploration_type
    from torchrl.objectives import ClipPPOLoss, group_optimizers
    from torchrl.objectives.value.advantages import GAE
    from torchrl.record import VideoRecorder
    from torchrl.record.loggers import generate_exp_name, get_logger
    from utils_ppo_occt import eval_model, make_env, make_ppo_models

    torch.set_float32_matmul_precision("high")

    device = cfg.optim.device
    if device in ("", None):
        if torch.cuda.is_available():
            device = "cuda:0"
        else:
            device = "cpu"
    device = torch.device(device)

    num_mini_batches = cfg.collector.frames_per_batch // cfg.loss.mini_batch_size
    total_network_updates = (
        (cfg.collector.total_frames // cfg.collector.frames_per_batch)
        * cfg.loss.ppo_epochs
        * num_mini_batches
    )

    compile_mode = None
    if cfg.compile.compile:
        compile_mode = cfg.compile.compile_mode
        if compile_mode in ("", None):
            if cfg.compile.cudagraphs:
                compile_mode = "default"
            else:
                compile_mode = "reduce-overhead"

    # Create models (é€‚é…12ç»´å½’ä¸€åŒ–è§‚æµ‹ï¼Œæ— éœ€ä¿®æ”¹ï¼Œç”±make_ppo_modelsè‡ªåŠ¨é€‚é…)
    actor, critic = make_ppo_models(cfg.env.env_name, device=device)

    # Create collectorï¼šä¿®æ”¹ç‚¹1 - å°è£…è®­ç»ƒç¯å¢ƒï¼Œè®¾ç½®vecnorm_frozen=Falseï¼ˆä¸å†»ç»“ï¼Œè‡ªåŠ¨æ›´æ–°ç»Ÿè®¡é‡ï¼‰
    collector = Collector(
        create_env_fn=lambda: make_env(  # ç”¨lambdaå°è£…ï¼Œä¼ é€’VecNormçŠ¶æ€å‚æ•°
            cfg.env.env_name,
            device,
            vecnorm_frozen=False  # è®­ç»ƒç¯å¢ƒï¼šä¸å†»ç»“VecNormï¼Œç»Ÿè®¡é‡éšè®­ç»ƒæ›´æ–°
        ),
        policy=actor,
        frames_per_batch=cfg.collector.frames_per_batch,
        total_frames=cfg.collector.total_frames,
        device=device,
        max_frames_per_traj=-1,
        compile_policy={"mode": compile_mode, "warmup": 1} if compile_mode else False,
        cudagraph_policy={"warmup": 10} if cfg.compile.cudagraphs else False,
    )

    # Create data buffer
    sampler = SamplerWithoutReplacement()
    data_buffer = TensorDictReplayBuffer(
        storage=LazyTensorStorage(
            cfg.collector.frames_per_batch,
            compilable=cfg.compile.compile,
            device=device,
        ),
        sampler=sampler,
        batch_size=cfg.loss.mini_batch_size,
        compilable=cfg.compile.compile,
    )

    # Create loss and adv modules
    adv_module = GAE(
        gamma=cfg.loss.gamma,
        lmbda=cfg.loss.gae_lambda,
        value_network=critic,
        average_gae=False,
        device=device,
        vectorized=not cfg.compile.compile,
    )

    loss_module = ClipPPOLoss(
        actor_network=actor,
        critic_network=critic,
        clip_epsilon=cfg.loss.clip_epsilon,
        loss_critic_type=cfg.loss.loss_critic_type,
        entropy_coeff=cfg.loss.entropy_coeff,
        critic_coeff=cfg.loss.critic_coeff,
        normalize_advantage=True,
    )

    # Create optimizers
    actor_optim = torch.optim.Adam(
        actor.parameters(), lr=torch.tensor(cfg.optim.lr, device=device), eps=1e-5
    )
    critic_optim = torch.optim.Adam(
        critic.parameters(), lr=torch.tensor(cfg.optim.lr, device=device), eps=1e-5
    )
    optim = group_optimizers(actor_optim, critic_optim)
    del actor_optim, critic_optim

    # Create loggerï¼šæ— éœ€ä¿®æ”¹ï¼Œä»…æ–°å¢VecNormç›¸å…³æ—¥å¿—ï¼ˆå¯é€‰ï¼‰
    logger = None
    if cfg.logger.backend:
        exp_name = generate_exp_name("PPO", f"{cfg.logger.exp_name}_{cfg.env.env_name}")
        logger = get_logger(
            cfg.logger.backend,
            logger_name="ppo",
            experiment_name=exp_name,
            swanlab_kwargs={
                "config": cfg,
                "group": cfg.logger.group_name,
                "project": cfg.logger.project_name
                or f"swanlab_{cfg.env.scenario_name}",
            },
        )
        logger_video = False
    else:
        logger_video = False

    # Create test environmentï¼šä¿®æ”¹ç‚¹2 - è¯„æµ‹ç¯å¢ƒè®¾ç½®vecnorm_frozen=Trueï¼ˆå†»ç»“ç»Ÿè®¡é‡ï¼Œä¿è¯ä¸€è‡´æ€§ï¼‰
    test_env = make_env(
        cfg.env.env_name,
        device,
        from_pixels=logger_video,
        render_mode=None,  # è‡ªå®šä¹‰rgb_arrayæ¸²æŸ“æ¨¡å¼
        enable_visualization=False,  # è‡ªå®šä¹‰å¯è§†åŒ–åŠŸèƒ½
        vecnorm_frozen=True  # è¯„æµ‹ç¯å¢ƒï¼šå†»ç»“VecNormï¼Œä¸æ›´æ–°ç»Ÿè®¡é‡ï¼Œä¿è¯è¯„æµ‹ç»“æœç¨³å®š
    )
    test_env.eval()

    # Updateå‡½æ•°ï¼šæ— éœ€ä¿®æ”¹ï¼Œé€‚é…å½’ä¸€åŒ–è§‚æµ‹è¾“å…¥
    def update(batch, num_network_updates):
        optim.zero_grad(set_to_none=True)
        # Linearly decrease the learning rate and clip epsilon
        alpha = torch.ones((), device=device)
        if cfg.optim.anneal_lr:
            alpha = 1 - (num_network_updates / total_network_updates)
            for group in optim.param_groups:
                group["lr"] = cfg.optim.lr * alpha
        if cfg.loss.anneal_clip_epsilon:
            loss_module.clip_epsilon.copy_(cfg.loss.clip_epsilon * alpha)
        num_network_updates = num_network_updates + 1

        # Forward pass PPO loss
        loss = loss_module(batch)
        critic_loss = loss["loss_critic"]
        actor_loss = loss["loss_objective"] + loss["loss_entropy"]
        total_loss = critic_loss + actor_loss

        # Backward pass
        total_loss.backward()

        # Update the networks
        optim.step()
        return loss.detach().set("alpha", alpha), num_network_updates

    if cfg.compile.compile:
        update = compile_with_warmup(update, mode=compile_mode, warmup=1)
        adv_module = compile_with_warmup(adv_module, mode=compile_mode, warmup=1)

    if cfg.compile.cudagraphs:
        warnings.warn(
            "CudaGraphModule is experimental and may lead to silently wrong results. Use with caution.",
            category=UserWarning,
        )
        update = CudaGraphModule(update, in_keys=[], out_keys=[], warmup=5)
        adv_module = CudaGraphModule(adv_module)

    # Main loop
    collected_frames = 0
    num_network_updates = torch.zeros((), dtype=torch.int64, device=device)
    pbar = tqdm.tqdm(
        total=cfg.collector.total_frames,
        desc="Training",
        leave=True,
        dynamic_ncols=True
    )

    eval_round_counter = 0  # åˆå§‹åŒ–è¯„æµ‹è½®æ¬¡ï¼Œæ¯æ¬¡è§¦å‘è¯„æµ‹è‡ªå¢

    # extract cfg variables
    cfg_loss_ppo_epochs = cfg.loss.ppo_epochs
    cfg_optim_anneal_lr = cfg.optim.anneal_lr
    cfg_optim_lr = torch.tensor(cfg.optim.lr, device=device)
    cfg_loss_anneal_clip_eps = cfg.loss.anneal_clip_epsilon
    cfg_loss_clip_epsilon = cfg.loss.clip_epsilon
    cfg_logger_test_interval = cfg.logger.test_interval
    cfg_logger_num_test_episodes = cfg.logger.num_test_episodes
    losses = TensorDict(batch_size=[cfg_loss_ppo_epochs, num_mini_batches])

    # ===================== ä¿®æ”¹ç‚¹3ï¼šå¢å¼ºCheckpointï¼Œä¿å­˜VecNormç»Ÿè®¡é‡ =====================
    def save_checkpoint(current_frames):
        """å°è£…Checkpointä¿å­˜é€»è¾‘ï¼Œé€‚é…cfgé…ç½®ï¼Œæ–°å¢VecNormç»Ÿè®¡é‡ä¿å­˜"""
        # å…³é”®ï¼šæå–è®­ç»ƒç¯å¢ƒçš„åŸå§‹TwoCarrierEnvå®ä¾‹ï¼Œè·å–VecNormç»Ÿè®¡é‡
        raw_train_env = None
        vecnorm_mean = np.zeros(12, dtype=np.float64)
        vecnorm_var = np.ones(12, dtype=np.float64) * 1e-4  # ä¸ç¯å¢ƒé»˜è®¤æœ€å°æ–¹å·®ä¸€è‡´
        vecnorm_frozen = False

        try:
            # è§£åŒ…torchrl Collectorçš„ç¯å¢ƒå®ä¾‹ï¼Œè·å–åŸå§‹TwoCarrierEnv
            train_env_instance = collector.env
            raw_train_env = train_env_instance.unwrapped
            while not isinstance(raw_train_env, TwoCarrierEnv) and raw_train_env is not None:
                raw_train_env = getattr(raw_train_env, "_env", raw_train_env.unwrapped)
            
            if raw_train_env is not None:
                # æå–VecNormç»Ÿè®¡é‡
                vecnorm_mean = raw_train_env.vecnorm_mean.copy()
                vecnorm_var = raw_train_env.vecnorm_var.copy()
                vecnorm_frozen = raw_train_env.vecnorm_frozen
        except Exception as e:
            print(f"âš ï¸ è·å–è®­ç»ƒç¯å¢ƒVecNormç»Ÿè®¡é‡å¤±è´¥ï¼ˆä¸å½±å“æ¨¡å‹ä¿å­˜ï¼‰ï¼š{e}")

        # æ„é€ Checkpointå­—å…¸ï¼Œæ–°å¢VecNormç›¸å…³å†…å®¹
        ckpt_dict = {
            "actor_state_dict": actor.state_dict(),
            "critic_state_dict": critic.state_dict(),
            "optim_state_dict": optim.state_dict(),
            "cfg": cfg,
            "collected_frames": current_frames,
            # æ–°å¢ï¼šVecNormç»Ÿè®¡é‡ï¼Œç”¨äºåç»­åŠ è½½æ—¶æ¢å¤å½’ä¸€åŒ–åˆ†å¸ƒ
            "vecnorm_mean": vecnorm_mean,
            "vecnorm_var": vecnorm_var,
            "vecnorm_frozen": vecnorm_frozen,
        }
        save_dir = cfg.checkpoint.checkpoint_dir
        os.makedirs(save_dir, exist_ok=True)
        save_path = os.path.join(save_dir, f"checkpoint_{current_frames}_frames.pt")
        torch.save(ckpt_dict, save_path)
        print(f"\nâœ… Checkpoint saved to: {save_path}")
        # æ‰“å°VecNormç»Ÿè®¡é‡ï¼Œç›‘æ§è®­ç»ƒæ”¶æ•›æƒ…å†µ
        if raw_train_env is not None:
            print(f"âœ… é™„å¸¦VecNormå‡å€¼å‰12ç»´ï¼š{vecnorm_mean[:12].round(6)}")
            print(f"âœ… é™„å¸¦VecNormæ–¹å·®å‰12ç»´ï¼š{vecnorm_var[:12].round(6)}")

    # ===================== æ–°å¢ï¼šCheckpointåŠ è½½å‡½æ•°ï¼ˆå¯é€‰ï¼Œç”¨äºåç»­ç»§ç»­è®­ç»ƒ/è¯„æµ‹ï¼‰ =====================
    def load_checkpoint(ckpt_path, target_env=None):
        """åŠ è½½Checkpointï¼Œæ¢å¤æ¨¡å‹å‚æ•°ä¸VecNormç»Ÿè®¡é‡"""
        if not os.path.exists(ckpt_path):
            raise FileNotFoundError(f"Checkpointæ–‡ä»¶ä¸å­˜åœ¨ï¼š{ckpt_path}")
        
        # åŠ è½½Checkpoint
        ckpt_dict = torch.load(ckpt_path, map_location=device)
        
        # æ¢å¤æ¨¡å‹ä¸ä¼˜åŒ–å™¨å‚æ•°
        actor.load_state_dict(ckpt_dict["actor_state_dict"])
        critic.load_state_dict(ckpt_dict["critic_state_dict"])
        optim.load_state_dict(ckpt_dict["optim_state_dict"])
        
        # æ¢å¤VecNormç»Ÿè®¡é‡ï¼ˆè‹¥ä¼ å…¥ç›®æ ‡ç¯å¢ƒï¼‰
        if target_env is not None:
            try:
                raw_env = target_env.unwrapped
                while not isinstance(raw_env, TwoCarrierEnv) and raw_env is not None:
                    raw_env = getattr(raw_env, "_env", raw_env.unwrapped)
                
                if raw_env is not None:
                    raw_env.vecnorm_mean = np.asarray(ckpt_dict["vecnorm_mean"], dtype=np.float64)
                    raw_env.vecnorm_var = np.asarray(ckpt_dict["vecnorm_var"], dtype=np.float64)
                    raw_env.vecnorm_frozen = ckpt_dict["vecnorm_frozen"]
                    print(f"âœ… å·²ä»Checkpointæ¢å¤VecNormç»Ÿè®¡é‡ï¼Œå†»ç»“çŠ¶æ€ï¼š{raw_env.vecnorm_frozen}")
            except Exception as e:
                print(f"âš ï¸ æ¢å¤VecNormç»Ÿè®¡é‡å¤±è´¥ï¼š{e}")
        
        # è¿”å›é…ç½®ä¸å·²è®­ç»ƒå¸§æ•°
        return ckpt_dict["cfg"], ckpt_dict["collected_frames"]

    # ç¤ºä¾‹ï¼šåŠ è½½é¢„è®­ç»ƒCheckpointç»§ç»­è®­ç»ƒï¼ˆæŒ‰éœ€å¯ç”¨ï¼Œéœ€åœ¨cfgä¸­é…ç½®load_ckpt_pathï¼‰
    # if hasattr(cfg.checkpoint, "load_ckpt_path") and cfg.checkpoint.load_ckpt_path:
    #     cfg, start_frames = load_checkpoint(cfg.checkpoint.load_ckpt_path, collector.env)
    #     collected_frames = start_frames
    #     last_saved_frames = start_frames
    #     print(f"âœ… å·²åŠ è½½é¢„è®­ç»ƒCheckpointï¼Œä» {start_frames} å¸§ç»§ç»­è®­ç»ƒ")

    # æå–ä¿å­˜é…ç½®ï¼ˆä»cfgè¯»å–ï¼Œé¿å…ç¡¬ç¼–ç ï¼‰
    save_interval = cfg.checkpoint.save_interval
    last_saved_frames = 0  # è®°å½•ä¸Šä¸€æ¬¡ä¿å­˜çš„å¸§æ•°ï¼Œé¿å…é‡å¤ä¿å­˜

    collector_iter = iter(collector)
    total_iter = len(collector)
    for i in range(total_iter):
        timeit.printevery(1000, total_iter, erase=True)

        with timeit("collecting"):
            data = next(collector_iter)

        metrics_to_log = {}
        frames_in_batch = data.numel()
        collected_frames += frames_in_batch
        pbar.update(frames_in_batch)

        # ===== è§¦å‘Checkpointä¿å­˜ï¼ˆé€»è¾‘ä¸å˜ï¼Œå¢å¼ºåè‡ªåŠ¨æºå¸¦VecNormç»Ÿè®¡é‡ï¼‰ =====
        if save_interval > 0:
            if collected_frames // save_interval > last_saved_frames // save_interval:
                save_checkpoint(collected_frames)
                last_saved_frames = collected_frames

        # Get training rewards and episode lengthsï¼šæ— éœ€ä¿®æ”¹
        episode_rewards = data["next", "episode_reward"][data["next", "done"]]
        if len(episode_rewards) > 0:
            episode_length = data["next", "step_count"][data["next", "done"]]
            metrics_to_log.update(
                {
                    "train/reward": episode_rewards.mean().item(),
                    "train/episode_length": episode_length.sum().item()
                    / len(episode_length),
                }
            )

        # PPO_occt_check.py çš„ main å‡½æ•°å¾ªç¯ä¸­

        # ================= ä¿®æ”¹ç‚¹5ï¼šæå–å¹¶ç»Ÿè®¡ Reward åˆ†é¡¹ =================
        try:
            # å®šä¹‰æˆ‘ä»¬è¦è®°å½•çš„ key (ä¸ç¯å¢ƒå’Œ utils ä¸­ä¸€è‡´)
            reward_keys = [
                "reward_r_force", 
                "reward_r_align", 
                "reward_r_smooth", 
                "reward_r_progress", 
                "reward_val_force"
            ]
            
            # TorchRL é€šå¸¸ä¼šå°† info ä¸­çš„ scalar æ•°æ®æå‡åˆ° "next" çš„ä¸€çº§ key
            # æˆ–è€…åœ¨ ("next", "info", key)
            
            # ä¼˜å…ˆæ£€æŸ¥ data["next"] ä¸‹æ˜¯å¦æœ‰è¿™äº› key
            next_td = data["next"]
            
            for key in reward_keys:
                # æ£€æŸ¥ key æ˜¯å¦å­˜åœ¨äº TensorDict
                if key in next_td.keys():
                    val_mean = next_td[key].float().mean().item()
                    metrics_to_log[f"reward_parts/{key}"] = val_mean
                # å¤‡é€‰ï¼šæœ‰æ—¶å€™ä¼šåœ¨ info åµŒå¥—ä¸‹
                elif ("info", key) in next_td.keys(include_nested=True):
                    val_mean = next_td["info", key].float().mean().item()
                    metrics_to_log[f"reward_parts/{key}"] = val_mean

        except Exception as e:
            if i == 0: print(f"âš ï¸ æå– Reward Details å¤±è´¥: {e}")
        # ===================================================================

        with timeit("training"):
            for j in range(cfg_loss_ppo_epochs):

                # Compute GAE
                with torch.no_grad(), timeit("adv"):
                    torch.compiler.cudagraph_mark_step_begin()
                    data = adv_module(data)
                    if compile_mode:
                        data = data.clone()

                with timeit("rb - extend"):
                    # Update the data buffer
                    data_reshape = data.reshape(-1)
                    data_buffer.extend(data_reshape)

                for k, batch in enumerate(data_buffer):
                    with timeit("update"):
                        torch.compiler.cudagraph_mark_step_begin()
                        loss, num_network_updates = update(
                            batch, num_network_updates=num_network_updates
                        )
                        loss = loss.clone()
                    num_network_updates = num_network_updates.clone()
                    losses[j, k] = loss.select(
                        "loss_critic", "loss_entropy", "loss_objective"
                    )

        # Get training losses and timesï¼šæ— éœ€ä¿®æ”¹
        losses_mean = losses.apply(lambda x: x.float().mean(), batch_size=[])
        for key, value in losses_mean.items():
            metrics_to_log.update({f"train/{key}": value.item()})
        metrics_to_log.update(
            {
                "train/lr": loss["alpha"] * cfg_optim_lr,
                "train/clip_epsilon": loss["alpha"] * cfg_loss_clip_epsilon
                if cfg_loss_anneal_clip_eps
                else cfg_loss_clip_epsilon,
            }
        )

        # Get test rewardsï¼šä¿®æ”¹ç‚¹4 - éªŒè¯è¯„æµ‹ç¯å¢ƒVecNormçŠ¶æ€ï¼Œç®€åŒ–è§†é¢‘é€»è¾‘
        with torch.no_grad(), set_exploration_type(
            ExplorationType.DETERMINISTIC
        ), timeit("eval"):
            if ((i - 1) * frames_in_batch) // cfg_logger_test_interval < (
                i * frames_in_batch
            ) // cfg_logger_test_interval:
                eval_round_counter += 1  # è‡ªå¢è¯„æµ‹è½®æ¬¡
                actor.eval()
                print(f"\n============= å¼€å§‹ç¬¬ {eval_round_counter} è½®è¯„æµ‹ =============")

                # ===== æ–°å¢ï¼šæ­¥éª¤1 - æå–è®­ç»ƒç¯å¢ƒçš„æœ€æ–°VecNormç»Ÿè®¡é‡ =====
                train_vecnorm_mean = np.zeros(12, dtype=np.float64)
                train_vecnorm_var = np.ones(12, dtype=np.float64) * 1e-4
                try:
                    # è§£åŒ…è®­ç»ƒç¯å¢ƒï¼Œè·å–æœ€æ–°çš„mean/varï¼ˆå¤ç”¨ä½ ç°æœ‰Checkpointä¸­çš„æå–é€»è¾‘ï¼‰
                    raw_train_env = collector.env.unwrapped
                    while not isinstance(raw_train_env, TwoCarrierEnv) and raw_train_env is not None:
                        raw_train_env = getattr(raw_train_env, "_env", raw_train_env.unwrapped)
                    
                    if raw_train_env is not None:
                        train_vecnorm_mean = raw_train_env.vecnorm_mean.copy()
                        train_vecnorm_var = raw_train_env.vecnorm_var.copy()
                        print(f"âœ… æå–åˆ°è®­ç»ƒç¯å¢ƒæœ€æ–°VecNormï¼šå‡å€¼å‰5ç»´ {train_vecnorm_mean[:5].round(6)}ï¼Œæ–¹å·®å‰5ç»´ {train_vecnorm_var[:5].round(6)}")
                except Exception as e:
                    print(f"âš ï¸ æå–è®­ç»ƒç¯å¢ƒVecNormå¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼ï¼š{e}")

                # ===== æ–°å¢ï¼šæ­¥éª¤2 - åŒæ­¥åˆ°test_envï¼Œå¹¶ç¡®ä¿å†»ç»“ =====
                try:
                    raw_test_env = test_env.unwrapped
                    while not isinstance(raw_test_env, TwoCarrierEnv) and raw_test_env is not None:
                        raw_test_env = getattr(raw_test_env, "_env", raw_test_env.unwrapped)
                    
                    if raw_test_env is not None:
                        # è¦†ç›–test_envçš„åˆå§‹mean/varä¸ºè®­ç»ƒç¯å¢ƒçš„æœ€æ–°å€¼
                        raw_test_env.vecnorm_mean = train_vecnorm_mean
                        raw_test_env.vecnorm_var = train_vecnorm_var
                        # å¼ºåˆ¶ç¡®è®¤å†»ç»“ï¼Œé¿å…æ„å¤–æ›´æ–°
                        raw_test_env.vecnorm_frozen = True
                        print(f"âœ… å·²å°†è®­ç»ƒç¯å¢ƒVecNormåŒæ­¥åˆ°test_envï¼Œä¸”ä¿æŒå†»ç»“çŠ¶æ€")
                except Exception as e:
                    print(f"âš ï¸ åŒæ­¥VecNormåˆ°test_envå¤±è´¥ï¼š{e}")
                
                # 1. æ‰§è¡ŒåŸæœ‰è¯„æµ‹é€»è¾‘ï¼ˆtest_envå·²å†»ç»“VecNormï¼Œeval_modelå†…éƒ¨éªŒè¯çŠ¶æ€ï¼‰
                test_rewards = eval_model(
                    actor, test_env, num_episodes=cfg_logger_num_test_episodes, eval_round=eval_round_counter
                )
                
                # 2. æ›´æ–°è¯„æµ‹æŒ‡æ ‡ï¼šä¿®æ­£test_rewardså·²ä¸ºå‡å€¼ï¼Œæ— éœ€å†è°ƒç”¨.mean()
                metrics_to_log.update(
                    {
                        "eval/reward": test_rewards.item() if isinstance(test_rewards, torch.Tensor) else test_rewards,
                        "eval/round": eval_round_counter,
                    }
                )
                
                actor.train()
                print(f"============= ç¬¬ {eval_round_counter} è½®è¯„æµ‹ç»“æŸ =============")

        if logger:
            metrics_to_log.update(timeit.todict(prefix="time"))
            metrics_to_log["time/speed"] = pbar.format_dict["rate"]
            for key, value in metrics_to_log.items():
                logger.log_scalar(key, value, collected_frames)

        collector.update_policy_weights_()

    # è®­ç»ƒç»“æŸåä¿å­˜æœ€ç»ˆCheckpoint
    if save_interval > 0:
        save_checkpoint(cfg.collector.total_frames)
    
    collector.shutdown()
    if not test_env.is_closed:
        test_env.close()
    pbar.close()
    print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼Œæ‰€æœ‰Checkpointå·²ä¿å­˜ï¼ˆå«VecNormç»Ÿè®¡é‡ï¼‰ï¼")


if __name__ == "__main__":
    main()